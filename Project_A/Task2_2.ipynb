{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task_2 - Q_2 - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distilling dataset with PAD:\n",
      "Distillation using subset size 30000 for epoch 1\n",
      "Distillation Epoch [1/1], Loss: 0.3792, Accuracy: 91.22%\n",
      "Distillation using subset size 31500 for epoch 2\n",
      "Distillation Epoch [1/1], Loss: 0.0974, Accuracy: 97.87%\n",
      "Distillation using subset size 33000 for epoch 3\n",
      "Distillation Epoch [1/1], Loss: 0.0771, Accuracy: 98.30%\n",
      "Distillation using subset size 34500 for epoch 4\n",
      "Distillation Epoch [1/1], Loss: 0.0622, Accuracy: 98.73%\n",
      "Distillation using subset size 36000 for epoch 5\n",
      "Distillation Epoch [1/1], Loss: 0.0577, Accuracy: 98.86%\n",
      "Distillation using subset size 37500 for epoch 6\n",
      "Distillation Epoch [1/1], Loss: 0.0535, Accuracy: 98.96%\n",
      "Distillation using subset size 39000 for epoch 7\n",
      "Distillation Epoch [1/1], Loss: 0.0485, Accuracy: 99.12%\n",
      "Distillation using subset size 40500 for epoch 8\n",
      "Distillation Epoch [1/1], Loss: 0.0458, Accuracy: 99.19%\n",
      "Distillation using subset size 42000 for epoch 9\n",
      "Distillation Epoch [1/1], Loss: 0.0434, Accuracy: 99.20%\n",
      "Distillation using subset size 43500 for epoch 10\n",
      "Distillation Epoch [1/1], Loss: 0.0413, Accuracy: 99.30%\n",
      "Distillation using subset size 45000 for epoch 11\n",
      "Distillation Epoch [1/1], Loss: 0.0368, Accuracy: 99.43%\n",
      "Distillation using subset size 46500 for epoch 12\n",
      "Distillation Epoch [1/1], Loss: 0.0367, Accuracy: 99.46%\n",
      "Distillation using subset size 48000 for epoch 13\n",
      "Distillation Epoch [1/1], Loss: 0.0363, Accuracy: 99.47%\n",
      "Distillation using subset size 49500 for epoch 14\n",
      "Distillation Epoch [1/1], Loss: 0.0362, Accuracy: 99.43%\n",
      "Distillation using subset size 51000 for epoch 15\n",
      "Distillation Epoch [1/1], Loss: 0.0335, Accuracy: 99.57%\n",
      "Distillation using subset size 52500 for epoch 16\n",
      "Distillation Epoch [1/1], Loss: 0.0327, Accuracy: 99.55%\n",
      "Distillation using subset size 54000 for epoch 17\n",
      "Distillation Epoch [1/1], Loss: 0.0326, Accuracy: 99.58%\n",
      "Distillation using subset size 55500 for epoch 18\n",
      "Distillation Epoch [1/1], Loss: 0.0300, Accuracy: 99.62%\n",
      "Distillation using subset size 57000 for epoch 19\n",
      "Distillation Epoch [1/1], Loss: 0.0288, Accuracy: 99.65%\n",
      "Distillation using subset size 58500 for epoch 20\n",
      "Distillation Epoch [1/1], Loss: 0.0284, Accuracy: 99.69%\n",
      "\n",
      "Training new model on the condensed dataset:\n",
      "Distillation Epoch [1/20], Loss: 1.1462, Accuracy: 99.11%\n",
      "Distillation Epoch [2/20], Loss: 0.1946, Accuracy: 99.89%\n",
      "Distillation Epoch [3/20], Loss: 0.0754, Accuracy: 99.98%\n",
      "Distillation Epoch [4/20], Loss: 0.0420, Accuracy: 100.00%\n",
      "Distillation Epoch [5/20], Loss: 0.0305, Accuracy: 100.00%\n",
      "Distillation Epoch [6/20], Loss: 0.0223, Accuracy: 100.00%\n",
      "Distillation Epoch [7/20], Loss: 0.0198, Accuracy: 100.00%\n",
      "Distillation Epoch [8/20], Loss: 0.0172, Accuracy: 100.00%\n",
      "Distillation Epoch [9/20], Loss: 0.0167, Accuracy: 100.00%\n",
      "Distillation Epoch [10/20], Loss: 0.0154, Accuracy: 100.00%\n",
      "Distillation Epoch [11/20], Loss: 0.0149, Accuracy: 100.00%\n",
      "Distillation Epoch [12/20], Loss: 0.0142, Accuracy: 100.00%\n",
      "Distillation Epoch [13/20], Loss: 0.0139, Accuracy: 100.00%\n",
      "Distillation Epoch [14/20], Loss: 0.0136, Accuracy: 100.00%\n",
      "Distillation Epoch [15/20], Loss: 0.0133, Accuracy: 100.00%\n",
      "Distillation Epoch [16/20], Loss: 0.0131, Accuracy: 100.00%\n",
      "Distillation Epoch [17/20], Loss: 0.0129, Accuracy: 100.00%\n",
      "Distillation Epoch [18/20], Loss: 0.0129, Accuracy: 100.00%\n",
      "Distillation Epoch [19/20], Loss: 0.0128, Accuracy: 100.00%\n",
      "Distillation Epoch [20/20], Loss: 0.0127, Accuracy: 100.00%\n",
      "\n",
      "Evaluating model trained on synthetic data:\n",
      "Test Accuracy on Real Test Data: 99.44%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.44"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from networks import ConvNet\n",
    "from utils import get_dataset\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# EL2N Score Function to calculate sample difficulty\n",
    "def calculate_el2n_score(model, data_loader):\n",
    "    scores = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            prob_outputs = torch.softmax(outputs, dim=1)\n",
    "            el2n_score = ((prob_outputs - torch.nn.functional.one_hot(targets, num_classes=prob_outputs.size(1)).float())**2).sum(dim=1)\n",
    "            scores.extend(el2n_score.cpu().tolist())\n",
    "    return scores\n",
    "\n",
    "# Scheduler for gradual data inclusion\n",
    "def data_scheduler(train_dataset, scores, initial_ratio=0.5, max_ratio=1.0, num_epochs=20):\n",
    "    sorted_indices = sorted(range(len(scores)), key=lambda i: scores[i])\n",
    "    total_samples = int(len(scores) * max_ratio)\n",
    "    increment = (total_samples - int(len(scores) * initial_ratio)) // num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        end_index = int(len(scores) * initial_ratio) + epoch * increment\n",
    "        yield torch.utils.data.Subset(train_dataset, sorted_indices[:end_index])\n",
    "\n",
    "# Define the training function for distillation\n",
    "def train_distilled_model(model, synthetic_data_loader, epochs=20, initial_lr=0.01):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss, correct = 0, 0\n",
    "        for inputs, targets in synthetic_data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        accuracy = 100. * correct / len(synthetic_data_loader.dataset)\n",
    "        scheduler.step()\n",
    "        print(f\"Distillation Epoch [{epoch + 1}/{epochs}], Loss: {total_loss/100:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "# Apply Dataset Distillation with PAD and create a synthetic dataset\n",
    "def distill_dataset_with_PAD(model, train_loader, epochs=10):\n",
    "    el2n_scores = calculate_el2n_score(model, train_loader)\n",
    "    scheduler = data_scheduler(train_loader.dataset, el2n_scores)\n",
    "    synthetic_data = []\n",
    "    synthetic_labels = []\n",
    "    \n",
    "    for epoch, subset in enumerate(scheduler):\n",
    "        print(f\"Distillation using subset size {len(subset)} for epoch {epoch + 1}\")\n",
    "        train_subset_loader = DataLoader(subset, batch_size=256, shuffle=True)\n",
    "        train_distilled_model(model, train_subset_loader, epochs=1)\n",
    "        \n",
    "        # Save synthetic images (condensed dataset)\n",
    "        for inputs, targets in train_subset_loader:\n",
    "            synthetic_data.append(inputs.cpu())\n",
    "            synthetic_labels.append(targets.cpu())\n",
    "    \n",
    "    synthetic_data = torch.cat(synthetic_data)\n",
    "    synthetic_labels = torch.cat(synthetic_labels)\n",
    "    return synthetic_data, synthetic_labels\n",
    "\n",
    "# Function to train a new model from scratch on synthetic dataset (Stage 2)\n",
    "def train_model_on_synthetic_data(synthetic_data, synthetic_labels, epochs=20):\n",
    "    # Convert synthetic data to TensorDataset format\n",
    "    synthetic_loader = DataLoader(TensorDataset(synthetic_data, synthetic_labels), batch_size=256, shuffle=True)\n",
    "    model = ConvNet(channel=1, num_classes=10, net_width=128, net_depth=3, net_act='relu', net_norm='batchnorm', net_pooling='avgpooling', im_size=(28, 28)).to(device)\n",
    "    return train_distilled_model(model, synthetic_loader, epochs)\n",
    "\n",
    "# Evaluate the model on real test data\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_accuracy = 100. * correct / total\n",
    "    print(f\"Test Accuracy on Real Test Data: {test_accuracy:.2f}%\")\n",
    "    return test_accuracy\n",
    "\n",
    "# Load data and initialize the model\n",
    "channel, im_size, num_classes, class_names, mean, std, train_dataset, test_dataset, test_loader = get_dataset('MNIST', './mnist_dataset')\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "model_mnist = ConvNet(channel=channel, num_classes=num_classes, net_width=128, net_depth=3, net_act='relu', net_norm='batchnorm', net_pooling='avgpooling', im_size=im_size).to(device)\n",
    "\n",
    "# Stage 1: Distill the dataset with PAD and create a synthetic dataset\n",
    "print(\"Distilling dataset with PAD:\")\n",
    "synthetic_data, synthetic_labels = distill_dataset_with_PAD(model_mnist, train_loader)\n",
    "\n",
    "# Stage 2: Train a new model on the synthetic data and evaluate on real test data\n",
    "print(\"\\nTraining new model on the condensed dataset:\")\n",
    "model_synthetic = train_model_on_synthetic_data(synthetic_data, synthetic_labels)\n",
    "\n",
    "# Evaluate the model trained on synthetic data\n",
    "print(\"\\nEvaluating model trained on synthetic data:\")\n",
    "evaluate_model(model_synthetic, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task_2 - Q_3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ConvNet-3 on MNIST with DATM:\n",
      "Epoch [1/20], Loss: 0.0174, Accuracy: 99.85%\n",
      "DATM Epoch [1/20], Matching Loss: 25.1445\n",
      "DATM Epoch [2/20], Matching Loss: 27.5291\n",
      "DATM Epoch [3/20], Matching Loss: 28.7153\n",
      "DATM Epoch [4/20], Matching Loss: 23.6025\n",
      "DATM Epoch [5/20], Matching Loss: 29.7281\n",
      "DATM Epoch [6/20], Matching Loss: 37.4621\n",
      "DATM Epoch [7/20], Matching Loss: 42.2686\n",
      "DATM Epoch [8/20], Matching Loss: 37.3069\n",
      "DATM Epoch [9/20], Matching Loss: 40.0144\n",
      "DATM Epoch [10/20], Matching Loss: 39.8121\n",
      "DATM Epoch [11/20], Matching Loss: 38.1626\n",
      "DATM Epoch [12/20], Matching Loss: 37.0455\n",
      "DATM Epoch [13/20], Matching Loss: 40.8722\n",
      "DATM Epoch [14/20], Matching Loss: 40.0883\n",
      "DATM Epoch [15/20], Matching Loss: 51.9066\n",
      "DATM Epoch [16/20], Matching Loss: 29.1467\n",
      "DATM Epoch [17/20], Matching Loss: 48.2282\n",
      "DATM Epoch [18/20], Matching Loss: 29.8836\n",
      "DATM Epoch [19/20], Matching Loss: 45.7632\n",
      "DATM Epoch [20/20], Matching Loss: 39.7854\n",
      "Epoch [2/20], Loss: 0.0164, Accuracy: 99.86%\n",
      "Epoch [3/20], Loss: 0.0148, Accuracy: 99.88%\n",
      "Epoch [4/20], Loss: 0.0133, Accuracy: 99.91%\n",
      "Epoch [5/20], Loss: 0.0120, Accuracy: 99.92%\n",
      "Epoch [6/20], Loss: 0.0112, Accuracy: 99.93%\n",
      "DATM Epoch [1/20], Matching Loss: 25.0557\n",
      "DATM Epoch [2/20], Matching Loss: 27.5207\n",
      "DATM Epoch [3/20], Matching Loss: 28.8225\n",
      "DATM Epoch [4/20], Matching Loss: 23.8045\n",
      "DATM Epoch [5/20], Matching Loss: 30.2792\n",
      "DATM Epoch [6/20], Matching Loss: 36.9280\n",
      "DATM Epoch [7/20], Matching Loss: 42.0007\n",
      "DATM Epoch [8/20], Matching Loss: 37.1376\n",
      "DATM Epoch [9/20], Matching Loss: 39.2602\n",
      "DATM Epoch [10/20], Matching Loss: 39.8856\n",
      "DATM Epoch [11/20], Matching Loss: 38.1718\n",
      "DATM Epoch [12/20], Matching Loss: 37.2240\n",
      "DATM Epoch [13/20], Matching Loss: 40.6200\n",
      "DATM Epoch [14/20], Matching Loss: 39.7826\n",
      "DATM Epoch [15/20], Matching Loss: 51.2783\n",
      "DATM Epoch [16/20], Matching Loss: 29.4177\n",
      "DATM Epoch [17/20], Matching Loss: 48.1462\n",
      "DATM Epoch [18/20], Matching Loss: 29.9464\n",
      "DATM Epoch [19/20], Matching Loss: 45.2457\n",
      "DATM Epoch [20/20], Matching Loss: 39.2514\n",
      "Epoch [7/20], Loss: 0.0100, Accuracy: 99.95%\n",
      "Epoch [8/20], Loss: 0.0094, Accuracy: 99.95%\n",
      "Epoch [9/20], Loss: 0.0084, Accuracy: 99.96%\n",
      "Epoch [10/20], Loss: 0.0083, Accuracy: 99.97%\n",
      "Epoch [11/20], Loss: 0.0075, Accuracy: 99.97%\n",
      "DATM Epoch [1/20], Matching Loss: 24.8520\n",
      "DATM Epoch [2/20], Matching Loss: 27.2417\n",
      "DATM Epoch [3/20], Matching Loss: 27.5966\n",
      "DATM Epoch [4/20], Matching Loss: 23.6585\n",
      "DATM Epoch [5/20], Matching Loss: 29.5721\n",
      "DATM Epoch [6/20], Matching Loss: 36.3660\n",
      "DATM Epoch [7/20], Matching Loss: 41.3326\n",
      "DATM Epoch [8/20], Matching Loss: 36.3037\n",
      "DATM Epoch [9/20], Matching Loss: 39.3239\n",
      "DATM Epoch [10/20], Matching Loss: 39.1473\n",
      "DATM Epoch [11/20], Matching Loss: 37.3045\n",
      "DATM Epoch [12/20], Matching Loss: 36.3776\n",
      "DATM Epoch [13/20], Matching Loss: 39.7478\n",
      "DATM Epoch [14/20], Matching Loss: 39.5593\n",
      "DATM Epoch [15/20], Matching Loss: 50.9054\n",
      "DATM Epoch [16/20], Matching Loss: 28.9785\n",
      "DATM Epoch [17/20], Matching Loss: 46.9077\n",
      "DATM Epoch [18/20], Matching Loss: 29.6998\n",
      "DATM Epoch [19/20], Matching Loss: 44.9290\n",
      "DATM Epoch [20/20], Matching Loss: 39.2446\n",
      "Epoch [12/20], Loss: 0.0068, Accuracy: 99.98%\n",
      "Epoch [13/20], Loss: 0.0063, Accuracy: 99.98%\n",
      "Epoch [14/20], Loss: 0.0060, Accuracy: 99.98%\n",
      "Epoch [15/20], Loss: 0.0057, Accuracy: 99.99%\n",
      "Epoch [16/20], Loss: 0.0055, Accuracy: 99.99%\n",
      "DATM Epoch [1/20], Matching Loss: 24.4744\n",
      "DATM Epoch [2/20], Matching Loss: 26.8999\n",
      "DATM Epoch [3/20], Matching Loss: 27.6519\n",
      "DATM Epoch [4/20], Matching Loss: 23.4237\n",
      "DATM Epoch [5/20], Matching Loss: 29.4723\n",
      "DATM Epoch [6/20], Matching Loss: 36.0728\n",
      "DATM Epoch [7/20], Matching Loss: 40.9915\n",
      "DATM Epoch [8/20], Matching Loss: 36.2854\n",
      "DATM Epoch [9/20], Matching Loss: 38.8034\n",
      "DATM Epoch [10/20], Matching Loss: 39.1390\n",
      "DATM Epoch [11/20], Matching Loss: 37.0464\n",
      "DATM Epoch [12/20], Matching Loss: 36.3190\n",
      "DATM Epoch [13/20], Matching Loss: 39.5130\n",
      "DATM Epoch [14/20], Matching Loss: 39.1953\n",
      "DATM Epoch [15/20], Matching Loss: 50.3962\n",
      "DATM Epoch [16/20], Matching Loss: 29.0577\n",
      "DATM Epoch [17/20], Matching Loss: 46.6496\n",
      "DATM Epoch [18/20], Matching Loss: 29.5527\n",
      "DATM Epoch [19/20], Matching Loss: 44.4493\n",
      "DATM Epoch [20/20], Matching Loss: 38.7819\n",
      "Epoch [17/20], Loss: 0.0054, Accuracy: 99.99%\n",
      "Epoch [18/20], Loss: 0.0053, Accuracy: 99.99%\n",
      "Epoch [19/20], Loss: 0.0052, Accuracy: 99.99%\n",
      "Epoch [20/20], Loss: 0.0051, Accuracy: 99.99%\n",
      "Test Accuracy: 99.45%\n",
      "FLOPs for a single forward pass: 4.86e+07\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from networks import ConvNet  \n",
    "from utils import get_dataset\n",
    "from torchinfo import summary  \n",
    "\n",
    "# Define FLOPs calculation function\n",
    "def calculate_flops(model, input_size):\n",
    "    model_summary = summary(model, input_size=input_size, verbose=0)\n",
    "    flops = model_summary.total_mult_adds  # Total multiply-adds\n",
    "    return flops\n",
    "\n",
    "# Initialize synthetic data as a parameter to be optimized\n",
    "def initialize_synthetic_data(input_shape, num_samples):\n",
    "    # Ensure input_shape is a tuple of integers\n",
    "    if not isinstance(input_shape, tuple) or not all(isinstance(dim, int) for dim in input_shape):\n",
    "        raise ValueError(\"input_shape must be a tuple of integers\")\n",
    "\n",
    "    # Create synthetic data on the specified device\n",
    "    synthetic_data = torch.randn(num_samples, *input_shape, device=device, requires_grad=True)\n",
    "    return synthetic_data\n",
    "\n",
    "# Define the DATM function\n",
    "def difficulty_aligned_trajectory_matching(model, synthetic_data, real_data_trajectory, ipc, start_epoch, end_epoch):\n",
    "    \n",
    "    # Define bounds based on IPC, start with early trajectories for low IPC\n",
    "    lower_bound, upper_bound = (0, min(10, end_epoch)) if ipc < 10 else (start_epoch, end_epoch)\n",
    "    \n",
    "    # Create optimizer for synthetic data\n",
    "    optimizer = optim.SGD([synthetic_data], lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(lower_bound, upper_bound):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass on synthetic data\n",
    "        synthetic_output = model(synthetic_data)\n",
    "        \n",
    "        # Ensure real_output has the same batch size as synthetic_output by slicing\n",
    "        real_output = real_data_trajectory[epoch].to(device)\n",
    "        if real_output.shape[0] > synthetic_output.shape[0]:\n",
    "            real_output = real_output[:synthetic_output.shape[0]]\n",
    "        \n",
    "        # Matching loss: minimize distance between synthetic and real model parameters\n",
    "        matching_loss = criterion(synthetic_output, real_output)\n",
    "        matching_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"DATM Epoch [{epoch + 1}/{upper_bound}], Matching Loss: {matching_loss.item():.4f}\")\n",
    "    return synthetic_data\n",
    "\n",
    "# Define training with DATM\n",
    "def train_with_datm(model, train_loader, real_data_trajectory, ipc, synthetic_data, epochs=20, initial_lr=0.01):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=initial_lr, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss, correct = 0, 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        accuracy = 100. * correct / len(train_loader.dataset)\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss/100:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Update synthetic data with DATM every few epochs\n",
    "        if epoch % 5 == 0:  # Adjust frequency of DATM update as needed\n",
    "            synthetic_data = difficulty_aligned_trajectory_matching(\n",
    "                model, synthetic_data, real_data_trajectory, ipc, 0, epochs\n",
    "            )\n",
    "\n",
    "    return model, synthetic_data\n",
    "\n",
    "# Define testing and FLOPs calculation\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Calculate FLOPs for a single forward pass\n",
    "    input_size = (1, *inputs.shape[1:]) \n",
    "    flops = calculate_flops(model, input_size)\n",
    "    print(f\"FLOPs for a single forward pass: {flops:.2e}\")\n",
    "    return accuracy, flops\n",
    "\n",
    "# Function to save real model trajectory\n",
    "def save_real_data_trajectory(model, train_loader, epochs):\n",
    "    trajectory = {}\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Save the model output at the current epoch to the trajectory\n",
    "        trajectory[epoch] = outputs.detach().clone()  \n",
    "    return trajectory\n",
    "\n",
    "# Load MNIST dataset using get_dataset\n",
    "channel, im_size, num_classes, class_names, mean, std, train_dataset, test_dataset, test_loader = get_dataset('MNIST', './mnist_dataset')\n",
    "\n",
    "# Set im_size as a tuple for compatibility with ConvNet\n",
    "im_size = (28, 28)  # for MNIST, since it's 28x28\n",
    "input_shape = (channel, im_size[0], im_size[1])\n",
    "\n",
    "# Create DataLoader with batch size of 256\n",
    "train_loader_mnist = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize ConvNet-3 for MNIST\n",
    "model_mnist = ConvNet(channel=channel, num_classes=num_classes, net_width=128, net_depth=3, net_act='relu', net_norm='batchnorm', net_pooling='avgpooling', im_size=im_size).to(device)\n",
    "\n",
    "# Save real data trajectory for comparison in DATM\n",
    "real_data_trajectory = save_real_data_trajectory(model_mnist, train_loader_mnist, epochs=20)\n",
    "\n",
    "# Initialize synthetic data for MNIST\n",
    "num_samples = 10  # Adjust based on IPC and dataset requirements\n",
    "synthetic_data = initialize_synthetic_data(input_shape, num_samples)\n",
    "\n",
    "# Train and evaluate ConvNet-3 on MNIST with DATM\n",
    "print(\"Training ConvNet-3 on MNIST with DATM:\")\n",
    "model_mnist, synthetic_data = train_with_datm(model_mnist, train_loader_mnist, real_data_trajectory, ipc=10, synthetic_data=synthetic_data, epochs=20)\n",
    "mnist_accuracy, mnist_flops = test_model(model_mnist, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
