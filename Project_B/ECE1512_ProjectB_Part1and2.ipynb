{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCTXpnS3sjzq",
        "outputId": "7cac8b2e-21f9-4ea0-890a-84e1bfb8b70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'mamba' already exists and is not an empty directory.\n",
            "/content/mamba/mamba\n",
            "Processing /content/mamba/mamba\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==2.2.4) (2.5.1+cu121)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==2.2.4) (1.11.1.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==2.2.4) (0.8.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==2.2.4) (4.46.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==2.2.4) (24.2)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.10/dist-packages (from mamba_ssm==2.2.4) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==2.2.4) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==2.2.4) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==2.2.4) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==2.2.4) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==2.2.4) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->mamba_ssm==2.2.4) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->mamba_ssm==2.2.4) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->mamba_ssm==2.2.4) (4.66.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->mamba_ssm==2.2.4) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm==2.2.4) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm==2.2.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm==2.2.4) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->mamba_ssm==2.2.4) (2024.8.30)\n",
            "Building wheels for collected packages: mamba_ssm\n",
            "  Building wheel for mamba_ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba_ssm: filename=mamba_ssm-2.2.4-cp310-cp310-linux_x86_64.whl size=323653202 sha256=c6edff068928b4ceacc6e820a163908c02294fe01dbcc05e95ed4530a5f81e77\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pgyxidbz/wheels/c8/52/57/e1d47a0b5671ea2c7e3d2105232f3861a6933be5d33b6abd22\n",
            "Successfully built mamba_ssm\n",
            "Installing collected packages: mamba_ssm\n",
            "  Attempting uninstall: mamba_ssm\n",
            "    Found existing installation: mamba-ssm 2.2.4\n",
            "    Uninstalling mamba-ssm-2.2.4:\n",
            "      Successfully uninstalled mamba-ssm-2.2.4\n",
            "Successfully installed mamba_ssm-2.2.4\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: torchprofile in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from torchprofile) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.4 in /usr/local/lib/python3.10/dist-packages (from torchprofile) (0.20.1+cu121)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->torchprofile) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.4->torchprofile) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.4->torchprofile) (11.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->torchprofile) (3.0.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0) (3.30.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0) (3.16.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0) (18.1.8)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->triton==2.0.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->triton==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->triton==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.10/dist-packages (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "### Get the MAMBA form GitHub \"https://github.com/state-spaces/mamba\" ###\n",
        "!git clone https://github.com/state-spaces/mamba.git\n",
        "%cd mamba\n",
        "!pip install .\n",
        "!pip install datasets transformers torchprofile\n",
        "!pip install triton==2.0.0\n",
        "!pip install thop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Import necessary libraries ###\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchprofile import profile_macs\n",
        "from thop import profile\n",
        "# Step 2: Set environment variable to try disabling Triton optimizations\n",
        "os.environ[\"DISABLE_TRITON\"] = \"1\"\n",
        "# Load the pre-trained MAMBA model\n",
        "model = MambaLMHeadModel.from_pretrained(\"state-spaces/mamba-2.8b-slimpj\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUETKrzIP4xF",
        "outputId": "1b7f6f2c-3d3d-4b8a-8721-e5d4febb3a54"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/mamba/mamba/mamba_ssm/utils/hf.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(resolved_archive_file, map_location=mapped_device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Set up the test dataset ###\n",
        "# Initialize the tokenizer using GPT-2's tokenizer (since Mamba tokenizer is not available)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Using GPT-2 tokenizer as an alternative\n",
        "\n",
        "# Add a pad token to the tokenizer (since GPT-2 doesn't have one by default)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Step 3: Load the WikiText-2 dataset\n",
        "# Load the WikiText-2 dataset\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")\n",
        "\n",
        "# Only use the training split (subset of the data for simplicity)\n",
        "texts = dataset['train']['text'][:1000]  # Use a smaller subset for quick testing or use the whole dataset\n",
        "\n",
        "# Step 4: Define a custom PyTorch Dataset class\n",
        "class WikiTextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "        return input_ids, attention_mask\n",
        "\n",
        "# Step 5: Create the Dataset and DataLoader\n",
        "wikitext_dataset = WikiTextDataset(texts, tokenizer)\n",
        "test_loader = DataLoader(wikitext_dataset, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "1BVU4cokQqLt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Evaluate the original Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize evaluation metrics\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "total_loss = 0\n",
        "total = 0\n",
        "correct = 0\n",
        "total_time = 0\n",
        "\n",
        "# Calculate FLOPs using thop\n",
        "input_sample = torch.randint(0, tokenizer.vocab_size, (1, 128)).to(device)\n",
        "macs, params = profile(model, inputs=(input_sample,))\n",
        "flops = 2 * macs  # FLOPs is typically 2 * MACs for neural networks\n",
        "\n",
        "print(f\"FLOPs: {flops:.2e}\")\n",
        "print(f\"Number of Parameters: {params}\")\n",
        "\n",
        "# Disable gradient calculations during evaluation to save memory\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (input_ids, attention_mask) in enumerate(test_loader):\n",
        "        # Move the inputs to the GPU if available\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        # Measure the time taken for the forward pass (inference time)\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Forward pass: compute predictions (without attention mask)\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits  # Assuming the output contains logits\n",
        "\n",
        "        end_time = time.time()\n",
        "        total_time += (end_time - start_time)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), input_ids.view(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # For testing accuracy, find the predicted tokens\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        # Calculate correct predictions\n",
        "        correct += (predictions == input_ids).sum().item()\n",
        "        total += input_ids.numel()\n",
        "\n",
        "        # Skip profiling of MACs for now due to Triton incompatibility issues\n",
        "\n",
        "# Calculate average loss and accuracy for the test set\n",
        "average_test_loss = total_loss / len(test_loader)\n",
        "accuracy = 100 * correct / total\n",
        "# average_inference_time = total_time / len(test_loader)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"Test Loss: {average_test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "# print(f\"Average Inference Time per Batch: {average_inference_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgaFYsbFstub",
        "outputId": "9e2f0491-01d7-463a-932d-744ccc7d6ecc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
            "FLOPs: 2.64e+11\n",
            "Number of Parameters: 1032130560.0\n",
            "Test Loss: 4.2953\n",
            "Test Accuracy: 63.47%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Prune the model using L1 unstructured pruning\n",
        "# Make a copy of the model to create a pruned version\n",
        "model_pruned = model\n",
        "\n",
        "# Apply L1 unstructured pruning to Linear and Conv1d layers\n",
        "pruned_layers_count = 0\n",
        "for name, module in model_pruned.named_modules():\n",
        "    # Only prune Linear and Conv1d layers\n",
        "    if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
        "        pruned_layers_count += 1\n",
        "\n",
        "# Print the total number of layers that have been pruned\n",
        "print(f\"Total number of layers pruned: {pruned_layers_count}\")\n",
        "\n",
        "# Step 8: Make pruning permanent\n",
        "for name, module in model_pruned.named_modules():\n",
        "    if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
        "        prune.remove(module, 'weight')\n",
        "\n",
        "# Step 9: Save the pruned model\n",
        "torch.save(model_pruned.state_dict(), 'model_pruned.pth')\n",
        "print(\"Pruned model has been saved as 'model_pruned.pth'.\")\n",
        "\n",
        "# Step 10: Evaluate FLOPs Before and After Pruning\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_pruned.to(device)\n",
        "\n",
        "# Prepare an input sample for FLOPs calculation\n",
        "input_sample = torch.randint(0, tokenizer.vocab_size, (1, 128)).to(device)\n",
        "\n",
        "# # Calculate FLOPs using thop for the original model\n",
        "macs_original, params_original = profile(model, inputs=(input_sample,))\n",
        "flops_original = 2 * macs_original  # FLOPs is typically 2 * MACs for neural networks\n",
        "\n",
        "# Calculate FLOPs using thop for the pruned model\n",
        "macs_pruned, params_pruned = profile(model_pruned, inputs=(input_sample,))\n",
        "flops_pruned = 2 * macs_pruned  # FLOPs is typically 2 * MACs for neural networks\n",
        "\n",
        "# Calculate Sparsity of the Pruned Model\n",
        "total_params_pruned = 0\n",
        "total_zero_params_pruned = 0\n",
        "\n",
        "for name, module in model_pruned.named_modules():\n",
        "    if hasattr(module, 'weight'):\n",
        "        weight = module.weight.detach().cpu().numpy()\n",
        "        total_params_pruned += weight.size\n",
        "        total_zero_params_pruned += (weight == 0).sum()\n",
        "\n",
        "sparsity = total_zero_params_pruned / total_params_pruned\n",
        "# print(f\"Pruned Model - Sparsity: {sparsity:.2%}\")\n",
        "\n",
        "# Calculate Effective FLOPs After Pruning\n",
        "effective_flops = (1 - sparsity) * flops_original\n",
        "print(f\"Pruned Model - FLOPs: {effective_flops:.2e}\")\n",
        "print(f\"Pruned Model - Number of Parameters: {params_pruned}\")\n",
        "\n",
        "# Step 11: Evaluate the Pruned Model\n",
        "# Set pruned model to evaluation mode\n",
        "model_pruned.eval()\n",
        "\n",
        "# Initialize evaluation metrics for pruned model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "total_loss = 0\n",
        "total = 0\n",
        "correct = 0\n",
        "total_time = 0\n",
        "\n",
        "# Disable gradient calculations during evaluation to save memory\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (input_ids, attention_mask) in enumerate(test_loader):\n",
        "        # Move the inputs to the GPU if available\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        # Measure the time taken for the forward pass (inference time)\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Forward pass: compute predictions (without attention mask)\n",
        "        outputs = model_pruned(input_ids)\n",
        "        logits = outputs.logits  # Assuming the output contains logits\n",
        "\n",
        "        end_time = time.time()\n",
        "        total_time += (end_time - start_time)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), input_ids.view(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # For testing accuracy, find the predicted tokens\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        # Calculate correct predictions\n",
        "        correct += (predictions == input_ids).sum().item()\n",
        "        total += input_ids.numel()\n",
        "\n",
        "# Calculate average loss and accuracy for the test set with pruned model\n",
        "average_test_loss = total_loss / len(test_loader)\n",
        "accuracy = 100 * correct / total\n",
        "# average_inference_time = total_time / len(test_loader)\n",
        "\n",
        "# Print the evaluation results for the pruned model\n",
        "print(f\"Pruned Model - Test Loss: {average_test_loss:.4f}\")\n",
        "print(f\"Pruned Model - Test Accuracy: {accuracy:.2f}%\")\n",
        "# print(f\"Pruned Model - Average Inference Time per Batch: {average_inference_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXBL3Hn6uT4u",
        "outputId": "62ac86aa-a4b5-477b-b04a-2fb0a43234d1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of layers pruned: 321\n",
            "Pruned model has been saved as 'model_pruned.pth'.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
            "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
            "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv1d'>.\n",
            "Pruned Model - FLOPs: 1.58e+11\n",
            "Pruned Model - Number of Parameters: 1032130560.0\n",
            "Pruned Model - Test Loss: 4.2953\n",
            "Pruned Model - Test Accuracy: 63.47%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSnYa3GcLLlr",
        "outputId": "58e19759-b0d4-4380-ab5d-c30dab69b18e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MambaLMHeadModel(\n",
            "  (backbone): MixerModel(\n",
            "    (embedding): Embedding(50280, 2560)\n",
            "    (layers): ModuleList(\n",
            "      (0-63): 64 x Block(\n",
            "        (norm): RMSNorm()\n",
            "        (mixer): Mamba(\n",
            "          (in_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
            "          (conv1d): Conv1d(5120, 5120, kernel_size=(4,), stride=(1,), padding=(3,), groups=5120)\n",
            "          (act): SiLU()\n",
            "          (x_proj): Linear(in_features=5120, out_features=192, bias=False)\n",
            "          (dt_proj): Linear(in_features=160, out_features=5120, bias=True)\n",
            "          (out_proj): Linear(in_features=5120, out_features=2560, bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm_f): RMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2560, out_features=50280, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OhFf9W2ULLov"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}