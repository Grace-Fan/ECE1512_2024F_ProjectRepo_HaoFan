{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from transformers import CLIPModel, AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Vision Encoder (Using CLIP)\n",
        "class VisionEncoder(nn.Module):\n",
        "    def __init__(self, clip_model_name=\"openai/clip-vit-base-patch32\"):\n",
        "        super(VisionEncoder, self).__init__()\n",
        "        self.clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
        "\n",
        "    def forward(self, images):\n",
        "        vision_outputs = self.clip_model.get_image_features(images)\n",
        "        return vision_outputs\n",
        "\n",
        "# Language Decoder (Using a Pretrained Language Model)\n",
        "class LanguageDecoder(nn.Module):\n",
        "    def __init__(self, language_model_name=\"gpt2\"):\n",
        "        super(LanguageDecoder, self).__init__()\n",
        "        self.language_model = AutoModelForCausalLM.from_pretrained(language_model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(language_model_name)\n",
        "\n",
        "        # Add a padding token\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "            self.language_model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "    def forward(self, text_inputs, attention_mask=None):\n",
        "        outputs = self.language_model(input_ids=text_inputs, attention_mask=attention_mask, output_hidden_states=True)\n",
        "        # Use the last hidden state for representation\n",
        "        hidden_states = outputs.hidden_states[-1]  # Shape: (batch_size, sequence_length, hidden_size)\n",
        "        return hidden_states\n",
        "\n",
        "# Additional Layers\n",
        "class IntermediateLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(IntermediateLayer, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.linear(x))\n",
        "\n",
        "# Combined Vision-Language Model\n",
        "class LlavaModel(nn.Module):\n",
        "    def __init__(self, vision_encoder, language_decoder, intermediate_layers=3):\n",
        "        super(LlavaModel, self).__init__()\n",
        "        self.vision_encoder = vision_encoder\n",
        "        self.language_decoder = language_decoder\n",
        "\n",
        "        # Adjusting hidden sizes dynamically\n",
        "        vision_output_size = self.vision_encoder.clip_model.config.projection_dim\n",
        "        language_output_size = self.language_decoder.language_model.config.n_embd\n",
        "\n",
        "        # Add intermediate layers\n",
        "        self.intermediate_layers = nn.ModuleList([\n",
        "            IntermediateLayer(vision_output_size + language_output_size, vision_output_size + language_output_size)\n",
        "            for _ in range(intermediate_layers)\n",
        "        ])\n",
        "\n",
        "        self.fusion_layer = nn.Linear(vision_output_size + language_output_size, language_output_size)\n",
        "\n",
        "    def forward(self, images, text_inputs):\n",
        "        # Process vision inputs\n",
        "        vision_features = self.vision_encoder(images)  # Shape: (batch_size, vision_output_size)\n",
        "\n",
        "        # Process language inputs\n",
        "        text_features = self.language_decoder(text_inputs)  # Shape: (batch_size, sequence_length, language_output_size)\n",
        "\n",
        "        # Reduce dimensions of text features\n",
        "        text_mean = text_features.mean(dim=1)  # Shape: (batch_size, language_output_size)\n",
        "\n",
        "        # Combine features (fusion logic)\n",
        "        combined_features = torch.cat([vision_features, text_mean], dim=-1)  # Shape: (batch_size, vision_output_size + language_output_size)\n",
        "\n",
        "        # Pass through intermediate layers\n",
        "        for layer in self.intermediate_layers:\n",
        "            combined_features = layer(combined_features)\n",
        "\n",
        "        fused_output = self.fusion_layer(combined_features)  # Shape: (batch_size, language_output_size)\n",
        "        return fused_output\n",
        "\n",
        "    def prune_largest_flop_layer(self):\n",
        "        # Find the intermediate layer with the largest FLOPs\n",
        "        max_flops = 0\n",
        "        max_layer_index = -1\n",
        "        input_size = self.intermediate_layers[0].linear.in_features\n",
        "\n",
        "        for i, layer in enumerate(self.intermediate_layers):\n",
        "            layer_flops = input_size * input_size\n",
        "            if layer_flops > max_flops:\n",
        "                max_flops = layer_flops\n",
        "                max_layer_index = i\n",
        "\n",
        "        # Remove the layer with the largest FLOPs\n",
        "        if max_layer_index >= 0:\n",
        "            self.intermediate_layers = nn.ModuleList([\n",
        "                layer for i, layer in enumerate(self.intermediate_layers) if i != max_layer_index\n",
        "            ])\n",
        "\n",
        "# Function to calculate time\n",
        "def calculate_time(model, inputs):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = model(*inputs)\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    return elapsed_time\n",
        "\n",
        "# Function to calculate FLOPs manually\n",
        "def calculate_flops(model, inputs):\n",
        "    flops = 0\n",
        "    batch_size = inputs[0].shape[0]\n",
        "\n",
        "    if isinstance(model, VisionEncoder):\n",
        "        vision_output_size = model.clip_model.config.projection_dim\n",
        "        flops = batch_size * vision_output_size * inputs[0].shape[2] * inputs[0].shape[3]  # Rough estimate for vision encoding\n",
        "\n",
        "    elif isinstance(model, LanguageDecoder):\n",
        "        seq_len = inputs[0].shape[1]\n",
        "        hidden_size = model.language_model.config.n_embd\n",
        "        flops = batch_size * seq_len * hidden_size * 2  # For self-attention and feed-forward layers\n",
        "\n",
        "    elif isinstance(model, LlavaModel):\n",
        "        # Vision FLOPs\n",
        "        vision_output_size = model.vision_encoder.clip_model.config.projection_dim\n",
        "        vision_flops = batch_size * vision_output_size * inputs[0].shape[2] * inputs[0].shape[3]\n",
        "\n",
        "        # Language FLOPs\n",
        "        seq_len = inputs[1].shape[1]\n",
        "        hidden_size = model.language_decoder.language_model.config.n_embd\n",
        "        language_flops = batch_size * seq_len * hidden_size * 2\n",
        "\n",
        "        # Intermediate Layers FLOPs\n",
        "        intermediate_layer_flops = 0\n",
        "        input_size = vision_output_size + hidden_size\n",
        "        for _ in model.intermediate_layers:\n",
        "            intermediate_layer_flops += batch_size * input_size * input_size  # Linear layer FLOPs\n",
        "\n",
        "        # Fusion Layer FLOPs\n",
        "        fusion_input_size = vision_output_size + hidden_size\n",
        "        fusion_flops = batch_size * fusion_input_size * hidden_size\n",
        "\n",
        "        flops = vision_flops + language_flops + intermediate_layer_flops + fusion_flops\n",
        "\n",
        "    return flops\n",
        "\n",
        "# Instantiate, Simulate, and Evaluate\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize Vision Encoder and Language Decoder\n",
        "    vision_encoder = VisionEncoder()\n",
        "    language_decoder = LanguageDecoder()\n",
        "\n",
        "    # Combine into LLaVA model with intermediate layers\n",
        "    llava_model = LlavaModel(vision_encoder, language_decoder, intermediate_layers=3)\n",
        "\n",
        "    # Simulate inputs\n",
        "    dummy_images = torch.rand((2, 3, 224, 224))  # Batch of 2 images, 3 channels, 224x224 resolution\n",
        "    dummy_text = [\"Describe this image.\", \"What do you see in the picture?\"]\n",
        "\n",
        "    # Tokenize text inputs\n",
        "    tokenizer = language_decoder.tokenizer\n",
        "    encoded_text = tokenizer(dummy_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Test Dataset\n",
        "    test_images = torch.rand((10, 3, 224, 224))  # Batch of 10 test images\n",
        "    test_text = [\"Test image {}\".format(i) for i in range(10)]\n",
        "    encoded_test_text = tokenizer(test_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Simulated labels for accuracy calculation\n",
        "    test_labels = torch.randint(0, 2, (10,))  # Random binary labels for testing\n",
        "\n",
        "    # Measure time and FLOPs for Vision Encoder\n",
        "    vision_time = calculate_time(vision_encoder, (test_images,))\n",
        "    vision_flops = calculate_flops(vision_encoder, (test_images,))\n",
        "    print(f\"Vision Encoder: Time = {vision_time:.4f}s, FLOPs = {vision_flops}\")\n",
        "\n",
        "    # Measure time and FLOPs for Language Decoder\n",
        "    lang_time = calculate_time(language_decoder, (encoded_test_text.input_ids,))\n",
        "    lang_flops = calculate_flops(language_decoder, (encoded_test_text.input_ids,))\n",
        "    print(f\"Language Decoder: Time = {lang_time:.4f}s, FLOPs = {lang_flops}\")\n",
        "\n",
        "    # Measure time, FLOPs for Full Model\n",
        "    full_time = calculate_time(llava_model, (test_images, encoded_test_text.input_ids))\n",
        "    full_flops = calculate_flops(llava_model, (test_images, encoded_test_text.input_ids))\n",
        "    print(f\"Full Model: Time = {full_time:.4f}s, FLOPs = {full_flops}\")\n",
        "\n",
        "    # Prune the layer with the largest FLOPs\n",
        "    llava_model.prune_largest_flop_layer()\n",
        "\n",
        "    # Measure time, FLOPs for the pruned model\n",
        "    pruned_time = calculate_time(llava_model, (test_images, encoded_test_text.input_ids))\n",
        "    pruned_flops = calculate_flops(llava_model, (test_images, encoded_test_text.input_ids))\n",
        "    print(f\"Pruned Model: Time = {pruned_time:.4f}s, FLOPs = {pruned_flops}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZq9kyhs8ytW",
        "outputId": "4ad67d14-ea31-455c-ba49-9e1fce615204"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vision Encoder: Time = 0.5615s, FLOPs = 256901120\n",
            "Language Decoder: Time = 0.0687s, FLOPs = 46080\n",
            "Full Model: Time = 0.6542s, FLOPs = 315929600\n",
            "Pruned Model: Time = 0.5701s, FLOPs = 299545600\n"
          ]
        }
      ]
    }
  ]
}