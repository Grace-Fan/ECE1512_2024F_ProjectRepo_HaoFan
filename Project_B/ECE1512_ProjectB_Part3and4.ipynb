{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using device: NVIDIA RTX 4000 Ada Generation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling before pruning:\n",
      "Profiling model on CUDA...\n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                   aten::to         0.01%       1.000us         0.01%       1.000us       0.027us            37  \n",
      "               aten::conv2d         0.03%       4.000us         1.10%     176.000us     176.000us             1  \n",
      "          aten::convolution         0.16%      25.000us         1.08%     172.000us     172.000us             1  \n",
      "         aten::_convolution         0.08%      12.000us         0.92%     147.000us     147.000us             1  \n",
      "    aten::cudnn_convolution         0.84%     135.000us         0.84%     135.000us     135.000us             1  \n",
      "              aten::flatten         0.03%       4.000us         0.09%      15.000us      15.000us             1  \n",
      "                 aten::view         6.76%       1.080ms         6.76%       1.080ms       2.177us           496  \n",
      "            aten::transpose         2.22%     354.000us         2.55%     407.000us       2.678us           152  \n",
      "           aten::as_strided         0.56%      89.000us         0.56%      89.000us       0.263us           339  \n",
      "               aten::expand         1.10%     175.000us         1.11%     178.000us       3.560us            50  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 15.978ms\n",
      "\n",
      "FLOPs before pruning: 63196672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-12-08 18:00:52 1793:1793 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2024-12-08 18:00:52 1793:1793 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2024-12-08 18:00:52 1793:1793 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before pruning: 0.49\n",
      "Model trained on toy task.\n",
      "Profiling after pruning:\n",
      "Profiling model on CUDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-12-08 18:19:41 1793:1793 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2024-12-08 18:19:41 1793:1793 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2024-12-08 18:19:41 1793:1793 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                   aten::to         0.01%       2.000us         0.01%       2.000us       0.054us            37  \n",
      "               aten::conv2d         0.03%       5.000us         0.97%     173.000us     173.000us             1  \n",
      "          aten::convolution         0.17%      30.000us         0.94%     168.000us     168.000us             1  \n",
      "         aten::_convolution         0.06%      11.000us         0.77%     138.000us     138.000us             1  \n",
      "    aten::cudnn_convolution         0.71%     127.000us         0.71%     127.000us     127.000us             1  \n",
      "              aten::flatten         0.03%       6.000us         0.11%      20.000us      20.000us             1  \n",
      "                 aten::view         6.62%       1.186ms         6.62%       1.186ms       2.391us           496  \n",
      "            aten::transpose         2.14%     384.000us         2.48%     444.000us       2.940us           151  \n",
      "           aten::as_strided         0.67%     120.000us         0.67%     120.000us       0.355us           338  \n",
      "               aten::expand         1.14%     204.000us         1.21%     217.000us       4.340us            50  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 17.919ms\n",
      "\n",
      "FLOPs after pruning: 59919872\n",
      "Accuracy after pruning: 0.51\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from transformers import CLIPModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Vision Encoder (Using CLIP)\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, clip_model_name=\"openai/clip-vit-base-patch32\"):\n",
    "        super(VisionEncoder, self).__init__()\n",
    "        self.clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
    "\n",
    "    def forward(self, images):\n",
    "        vision_outputs = self.clip_model.get_image_features(images)\n",
    "        return vision_outputs\n",
    "\n",
    "# Language Decoder (Using a Pretrained Language Model)\n",
    "class LanguageDecoder(nn.Module):\n",
    "    def __init__(self, language_model_name=\"gpt2\"):\n",
    "        super(LanguageDecoder, self).__init__()\n",
    "        self.language_model = AutoModelForCausalLM.from_pretrained(language_model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(language_model_name)\n",
    "        \n",
    "        # Add a padding token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            self.language_model.resize_token_embeddings(len(self.tokenizer))\n",
    "\n",
    "    def forward(self, text_inputs, attention_mask=None):\n",
    "        outputs = self.language_model(input_ids=text_inputs, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1]  # Shape: (batch_size, sequence_length, hidden_size)\n",
    "        return hidden_states\n",
    "\n",
    "# Additional Layers\n",
    "class IntermediateLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(IntermediateLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x))\n",
    "\n",
    "# Combined Vision-Language Model\n",
    "class LlavaModel(nn.Module):\n",
    "    def __init__(self, vision_encoder, language_decoder, intermediate_layers=3):\n",
    "        super(LlavaModel, self).__init__()\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.language_decoder = language_decoder\n",
    "\n",
    "        vision_output_size = self.vision_encoder.clip_model.config.projection_dim\n",
    "        language_output_size = self.language_decoder.language_model.config.n_embd\n",
    "\n",
    "        self.intermediate_layers = nn.ModuleList([\n",
    "            IntermediateLayer(vision_output_size + language_output_size, vision_output_size + language_output_size)\n",
    "            for _ in range(intermediate_layers)\n",
    "        ])\n",
    "\n",
    "        self.fusion_layer = nn.Linear(vision_output_size + language_output_size, language_output_size)\n",
    "        self.classification_layer = nn.Linear(language_output_size, 1)  # Final output for binary classification\n",
    "\n",
    "    def forward(self, images, text_inputs):\n",
    "        vision_features = self.vision_encoder(images)\n",
    "        text_features = self.language_decoder(text_inputs).mean(dim=1)\n",
    "        combined_features = torch.cat([vision_features, text_features], dim=-1)\n",
    "\n",
    "        for layer in self.intermediate_layers:\n",
    "            combined_features = layer(combined_features)\n",
    "\n",
    "        fused_output = self.fusion_layer(combined_features)\n",
    "        classification_output = self.classification_layer(fused_output)  # Shape: (batch_size, 1)\n",
    "        return classification_output\n",
    "\n",
    "    def calculate_flops_per_layer(self, batch_size):\n",
    "        flops = []\n",
    "        vision_output_size = self.vision_encoder.clip_model.config.projection_dim\n",
    "        language_output_size = self.language_decoder.language_model.config.n_embd\n",
    "\n",
    "        # Calculate FLOPs for each intermediate layer\n",
    "        for layer in self.intermediate_layers:\n",
    "            input_size = vision_output_size + language_output_size\n",
    "            layer_flops = batch_size * input_size * input_size\n",
    "            flops.append(layer_flops)\n",
    "\n",
    "        return flops\n",
    "\n",
    "    def prune_max_flop_layer(self, batch_size):\n",
    "        flops = self.calculate_flops_per_layer(batch_size)\n",
    "        if flops:\n",
    "            max_flop_index = flops.index(max(flops))\n",
    "            del self.intermediate_layers[max_flop_index]\n",
    "\n",
    "# Function for Profiling\n",
    "def profile_model(model, inputs):\n",
    "    device = \"CUDA\" if torch.cuda.is_available() else \"CPU\"\n",
    "    print(f\"Profiling model on {device}...\")\n",
    "    with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./log\")) as prof:\n",
    "        model(*inputs)\n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "# Function to calculate FLOPs\n",
    "def calculate_flops(model, inputs):\n",
    "    flops = 0\n",
    "    batch_size = inputs[0].size(0)\n",
    "\n",
    "    # Vision FLOPs\n",
    "    vision_output_size = model.vision_encoder.clip_model.config.projection_dim\n",
    "    flops += batch_size * vision_output_size * inputs[0].size(2) * inputs[0].size(3)\n",
    "\n",
    "    # Language FLOPs\n",
    "    seq_len = inputs[1].size(1)\n",
    "    language_output_size = model.language_decoder.language_model.config.n_embd\n",
    "    flops += batch_size * seq_len * language_output_size * 2\n",
    "\n",
    "    # Intermediate Layers FLOPs\n",
    "    for layer in model.intermediate_layers:\n",
    "        input_size = vision_output_size + language_output_size\n",
    "        flops += batch_size * input_size * input_size\n",
    "\n",
    "    # Fusion Layer FLOPs\n",
    "    fusion_input_size = vision_output_size + language_output_size\n",
    "    flops += batch_size * fusion_input_size * language_output_size\n",
    "\n",
    "    # Classification Layer FLOPs\n",
    "    flops += batch_size * language_output_size * 1\n",
    "\n",
    "    return flops\n",
    "\n",
    "# Toy Task for Lossy-ness Measurement\n",
    "def train_on_toy_task(model, dataloader, criterion, optimizer, epochs=2):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            images = images.cuda() if torch.cuda.is_available() else images\n",
    "            labels = (labels % 2).float().unsqueeze(1).cuda() if torch.cuda.is_available() else (labels % 2).float().unsqueeze(1)  # Convert to binary and match output shape\n",
    "            dummy_text = [\"Dummy text input\"] * images.size(0)\n",
    "            encoded_text = model.language_decoder.tokenizer(dummy_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "            encoded_text = encoded_text.cuda() if torch.cuda.is_available() else encoded_text\n",
    "\n",
    "            outputs = model(images, encoded_text)  # Output shape: (batch_size, 1)\n",
    "            loss = criterion(outputs, labels)  # Loss expects shape (batch_size, 1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model\n",
    "\n",
    "# Function to measure accuracy\n",
    "def calculate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.cuda() if torch.cuda.is_available() else images\n",
    "            labels = (labels % 2).float().unsqueeze(1).cuda() if torch.cuda.is_available() else (labels % 2).float().unsqueeze(1)  # Convert to binary\n",
    "            dummy_text = [\"Dummy text input\"] * images.size(0)\n",
    "            encoded_text = model.language_decoder.tokenizer(dummy_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "            encoded_text = encoded_text.cuda() if torch.cuda.is_available() else encoded_text\n",
    "\n",
    "            outputs = torch.sigmoid(model(images, encoded_text))  # Sigmoid for binary classification\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA is not available. Falling back to CPU.\")\n",
    "    else:\n",
    "        print(f\"CUDA is available. Using device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    vision_encoder = VisionEncoder()\n",
    "    language_decoder = LanguageDecoder()\n",
    "    llava_model = LlavaModel(vision_encoder, language_decoder, intermediate_layers=3)\n",
    "    llava_model = llava_model.cuda() if torch.cuda.is_available() else llava_model\n",
    "\n",
    "    dummy_images = torch.rand((2, 3, 224, 224)).cuda() if torch.cuda.is_available() else torch.rand((2, 3, 224, 224))\n",
    "    dummy_text = [\"What is in this image?\", \"Describe the objects.\"]\n",
    "    tokenizer = language_decoder.tokenizer\n",
    "    encoded_text = tokenizer(dummy_text, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n",
    "    encoded_text = encoded_text.cuda() if torch.cuda.is_available() else encoded_text\n",
    "\n",
    "    print(\"Profiling before pruning:\")\n",
    "    profile_model(llava_model, (dummy_images, encoded_text))\n",
    "    print(f\"FLOPs before pruning: {calculate_flops(llava_model, (dummy_images, encoded_text))}\")\n",
    "\n",
    "    # Load toy dataset (MNIST) with transformation for RGB\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1))  # Convert grayscale (1 channel) to RGB (3 channels)\n",
    "    ])\n",
    "    mnist_data = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(mnist_data, batch_size=16, shuffle=True)\n",
    "\n",
    "    # Define criterion and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(llava_model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Calculate accuracy before pruning\n",
    "    accuracy_before = calculate_accuracy(llava_model, dataloader)\n",
    "    print(f\"Accuracy before pruning: {accuracy_before:.2f}\")\n",
    "\n",
    "    # Train on toy dataset\n",
    "    trained_model = train_on_toy_task(llava_model, dataloader, criterion, optimizer)\n",
    "    print(\"Model trained on toy task.\")\n",
    "\n",
    "    llava_model.prune_max_flop_layer(batch_size=16)\n",
    "\n",
    "    print(\"Profiling after pruning:\")\n",
    "    profile_model(llava_model, (dummy_images, encoded_text))\n",
    "    print(f\"FLOPs after pruning: {calculate_flops(llava_model, (dummy_images, encoded_text))}\")\n",
    "\n",
    "    # Calculate accuracy after pruning\n",
    "    accuracy_after = calculate_accuracy(llava_model, dataloader)\n",
    "    print(f\"Accuracy after pruning: {accuracy_after:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
